{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6 - Scikit-learn\n",
    "\n",
    "Today we're going to cover the popular machine learning library [Scikit-learn](http://scikit-learn.org/stable/)\n",
    "\n",
    "First:\n",
    "```bash\n",
    "conda install scikit-learn\n",
    "```\n",
    "\n",
    "Note, when importing scikit-learn, you do\n",
    "```python\n",
    "import sklearn # not \"scikit-learn\"\n",
    "```\n",
    "\n",
    "This library can do just about anything you would learn in an introductory machine learning class (although it doesn't really do deep learning).  This includes:\n",
    "\n",
    "* Regression\n",
    "* SVMs\n",
    "* Decision trees/random forests\n",
    "* dimensionality reduction\n",
    "* clustering\n",
    "* validation\n",
    "* ...\n",
    "\n",
    "Supervised learning:\n",
    "* Regression and classification methods\n",
    "* All types of models: logistic regression, ridge, SVM, lasso regression, decision trees... up to Neural networks (no GPU support)\n",
    "* Stochastic Gradient Descent, Nearest-Neighbors,\n",
    "* Also features semi-supervised learning, ensemble methods, feature selection methods, Naive Bayes, and Isotonic Regression\n",
    "\n",
    "Unsupervised learning:\n",
    "* Gaussian Mixture Models, Manifold Learning\n",
    "* Clustering, Bi-clustering\n",
    "* PCA, LDA, Outlier detection, Covariance estimation\n",
    "\n",
    "You may wish to check out [some examples](http://scikit-learn.org/stable/auto_examples/)\n",
    "\n",
    "As usual, this class will assume you have some passing familiarity with at least something above, since this class isn't really trying to tell you *why* you may wish to classify something or do a regression, just *how* to do it in Python (or at least point you in that direction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading an example dataset\n",
    "\n",
    "First we will load some data to play with. The data we will use is a very simple\n",
    "flower database known as the Iris dataset.\n",
    "\n",
    "We have 150 observations of the iris flower specifying some measurements: \n",
    "\n",
    "- sepal length, sepal width, petal length and petal width together with its subtype:\n",
    "*Iris setosa*, *Iris versicolor*, *Iris virginica*.\n",
    "\n",
    "Yes, we saw this last class as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is stored in the `.data` member, which is a `(n_samples, n_features)`\n",
    "array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_string = '\\n' + '--'*25 + '\\n'\n",
    "print(iris.keys(), end = end_string)\n",
    "print(iris.target.shape, end = end_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class of each observation is stored in the `.target` attribute of the\n",
    "dataset. This is an integer 1D array of length `n_samples`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target.shape)\n",
    "np.unique(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Scikit-learn Paradigm\n",
    "\n",
    "Almost everything you do in scikit learn will be a variation of the same basic pattern, regardless of the specifics of what you're actually doing\n",
    "\n",
    "1. Load the model class\n",
    "2. Initialize a model (this is where you specify parameters)\n",
    "3. Fit your model to data\n",
    "4. (Context dependent) - predict, visualize, explore your fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Classification\n",
    "\n",
    "We'll start with a nearest neighbor classifier.\n",
    "\n",
    "## k-Nearest neighbors classifier\n",
    "\n",
    "The simplest possible classifier is the nearest neighbor: given a new\n",
    "observation, take the label of the training samples closest to it in\n",
    "*n*-dimensional space, where *n* is the number of *features* in each sample.\n",
    "\n",
    "The k-nearest neighbors classifier internally uses an algorithm based on\n",
    "ball trees to represent the samples it is trained on.\n",
    "\n",
    "![image](./img/iris_knn.png)\n",
    "\n",
    "Note that most functionality in `sklearn` lives in modules, so you'll need to do something like\n",
    "```python\n",
    "from sklearn import neighbors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn import neighbors # access to model class\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier() # initialize a model (default parameters)\n",
    "\n",
    "knn.fit(iris.data, iris.target) # fit the model\n",
    "\n",
    "knn.predict([[0.1, 0.2, 0.3, 0.4]]) # do something with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set and testing set\n",
    "\n",
    "When experimenting with learning algorithms, it is important not to test the\n",
    "prediction of an estimator on the data used to fit the estimator. \n",
    "\n",
    "Indeed, with the kNN estimator, we would always get perfect prediction on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manually\n",
    "perm = np.random.permutation(iris.target.size)\n",
    "iris.data = iris.data[perm]\n",
    "iris.target = iris.target[perm]\n",
    "knn.fit(iris.data[:100], iris.target[:100])\n",
    "knn.score(iris.data[100:], iris.target[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preferred\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# split holding out 40 % \n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# We are drastically reducing the size of our training data, better to do k-fold cross validation \n",
    "scores = cross_val_score(knn, iris.data, iris.target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Classifiers\n",
    "\n",
    "SVM, decision trees, random forests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC() # you can pass in various key-word arguments\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "## K-means\n",
    "\n",
    "A simple clustering algorithm is k-means. This divides a set into *k*\n",
    "clusters, assigning each observation to a cluster so as to minimize the distance\n",
    "of that observation (in *n*-dimensional space) to the cluster's mean; the means\n",
    "are then recomputed. This operation is run iteratively until the clusters\n",
    "converge, for a maximum for `max_iter` rounds.\n",
    "\n",
    "(An alternative implementation of k-means is available in SciPy's `cluster`\n",
    "package. The `scikit-learn` implementation differs from that by offering an\n",
    "object API and several additional features, including smart initialization.)\n",
    "\n",
    "[sklearn kmeans](http://scikit-learn.org/stable/modules/clustering.html#k-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "labels= k_means.fit_predict(iris.data)\n",
    "print(labels[::10])\n",
    "print(iris.target[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Clustering Methods\n",
    "\n",
    "Most standard clustering algorithms are available in scikit-learn\n",
    "\n",
    "[clustering in sklearn](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglom = cluster.AgglomerativeClustering(n_clusters=3, linkage=\"single\")\n",
    "labels= k_means.fit_predict(iris.data)\n",
    "print(labels[::10])\n",
    "print(iris.target[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "In regression, we're looking to predict a response $y$ from data $X$.  Scikit learn implements most basic regression models, as well as some less standard ones.\n",
    "\n",
    "If you're familiar with R, the models should be familiar, but the API is new.\n",
    "\n",
    "[sklearn regression](http://scikit-learn.org/stable/modules/linear_model.html)\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Let's do something a little less trivial than what we have above.  We'll use the pandas, patsy, and statsmodels packages\n",
    "\n",
    "```bash\n",
    "conda install pandas statsmodels patsy\n",
    "```\n",
    "\n",
    "[statsmodels](https://www.statsmodels.org/stable/index.html) is another python library for statistical estimation problems.  We'll use it for an included dataset.\n",
    "\n",
    "[patsy](https://patsy.readthedocs.io/en/latest/) helps specify statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following data set, which contains data about the incidence of extramarital affairs in marriages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm.datasets.fair.SOURCE)\n",
    "print(sm.datasets.fair.NOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = sm.datasets.fair.load_pandas().data\n",
    "\n",
    "# add \"affair\" column: 1 represents having affairs, 0 represents not\n",
    "data['affair'] = (data.affairs > 0).astype(int)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Affair proportion by children: \\n \\n {}\\n\".format(data.groupby('children')['affair'].mean()))\n",
    "print(\"Affair proportion by age: \\n \\n {}\".format(data.groupby('age')['affair'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups marriages by how they are rated by the couple\n",
    "data.groupby('rate_marriage').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll visualize a histogram of education levels of the couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.educ.hist()\n",
    "plt.title('Histogram of Education')\n",
    "plt.xlabel('Education Level')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rate_marriage.hist()\n",
    "plt.title('Histogram of Marriage Rating')\n",
    "plt.xlabel('Marriage Rating')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data.corr())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data, figsize=(15, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affair_yrs_married = pd.crosstab(data.yrs_married, data.affair.astype(bool))\n",
    "affair_yrs_married.div(affair_yrs_married.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, figsize=(10,10))\n",
    "plt.title('Affair Percentage by Years Married')\n",
    "plt.xlabel('Years Married')\n",
    "plt.ylim([0,1.25])\n",
    "_ = plt.ylabel('Percentage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the [patsy](https://patsy.readthedocs.io/en/latest/) library to create some data matrices from our data frames.\n",
    "\n",
    "Our features will be all the (non-affair) features in the original dataset, and the response will be the presence of an affair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes with an intercept column and dummy variables for\n",
    "# occupation and occupation_husb\n",
    "y, X = dmatrices('affair ~ rate_marriage + age + yrs_married + children + \\\n",
    "                  religious + educ + C(occupation) + C(occupation_husb)',\n",
    "                  data, return_type=\"dataframe\")\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.rename(columns = {'C(occupation)[T.2.0]':'occ_2',\n",
    "                        'C(occupation)[T.3.0]':'occ_3',\n",
    "                        'C(occupation)[T.4.0]':'occ_4',\n",
    "                        'C(occupation)[T.5.0]':'occ_5',\n",
    "                        'C(occupation)[T.6.0]':'occ_6',\n",
    "                        'C(occupation_husb)[T.2.0]':'occ_husb_2',\n",
    "                        'C(occupation_husb)[T.3.0]':'occ_husb_3',\n",
    "                        'C(occupation_husb)[T.4.0]':'occ_husb_4',\n",
    "                        'C(occupation_husb)[T.5.0]':'occ_husb_5',\n",
    "                        'C(occupation_husb)[T.6.0]':'occ_husb_6'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten y dataframe into a response array in numpy\n",
    "y = np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to fit a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a logistic regression model, and fit with X and y\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X, y)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict class labels for the test set\n",
    "predicted = model2.predict(X_test)\n",
    "print(\"Predicted {} affairs in {} points\".format(predicted.sum(), X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate class probabilities\n",
    "probs = model2.predict_proba(X_test)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create an [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) to evaluate the model's ability to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate evaluation metrics\n",
    "\n",
    "roc_auc = metrics.roc_auc_score(y_test, probs[:, 1])\n",
    "acc = metrics.accuracy_score(y_test, predicted)\n",
    "print(\"Accuracy score: {}\".format(acc))\n",
    "print(\"ROC-AUC score {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, probs[:, 1], pos_label=1)\n",
    "plt.figure(figsize=(10,10))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix= metrics.confusion_matrix(y_test, predicted)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "report =metrics.classification_report(y_test, predicted)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribution\n",
    "\n",
    "Portions of this notebook ar a Jupyter Notebook port of the `scikit-learn` lecture from the\n",
    "open source [Scipy Lecture Notes][scipy-lec-notes] by Fabian Pedregosa and Gael\n",
    "Varoquaux.\n",
    "\n",
    "[scipy-lec-notes]: http://www.scipy-lectures.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6-cme193)",
   "language": "python",
   "name": "cme193"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
