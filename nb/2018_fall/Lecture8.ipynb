{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME 193 - Lecture 8\n",
    "\n",
    "Here's what you've seen over the past 7 lectures:\n",
    "* Python Language Basics\n",
    "* NumPy - Arrays/Linear Algebra\n",
    "* SciPy - Sparse Linear Algebra/Optimization\n",
    "* DataFrames - Reading & Maniputlating tabular data\n",
    "* Scikit learn - Machine Learning Models & use with data\n",
    "* Ortools - More Optimization\n",
    "\n",
    "You've now seen some tools for scientific computing in Python.  How you add to them and what you do with them is up to you!\n",
    "\n",
    "![python](https://imgs.xkcd.com/comics/python.png)\n",
    "\n",
    "(Maybe you've also had a bit of [this](https://xkcd.com/1987/) experience)\n",
    "\n",
    "## Today\n",
    "\n",
    "1. We'll revisit object oriented programming in Python\n",
    "2. We'll talk a bit about image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Oriented Programming - II\n",
    "\n",
    "Recall some of the basic terminology of [object oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming)\n",
    "* **Classes** are templates for objects (e.g., \"the Integers\" is a class)\n",
    "* **Objects** are specific instances of a class (e.g., \"2 is an integer\")\n",
    "* **Methods** are fuctions associated to objects of a class\n",
    "    * the \"the square of 2\" may be expressed as `2.square()` (returns 4)\n",
    "    * the \"addition of 1 to 2\" may be expressed as `2.add(1)` (returns 3)\n",
    "    * the \"name of 2\" may be expressed as `2.name()` (returns \"two\")\n",
    "\n",
    "Today we'll use an extended example of univariate functions\n",
    "$$f:\\mathbb{R} \\to \\mathbb{R}$$\n",
    "to see how you might use object oriented programming for something like automatic differentiation, classical machine learning, or deep learning.  Yes - you can maybe use a library like [Tensorflow](https://www.tensorflow.org/), [Keras](https://keras.io/), or [PyTorch](https://pytorch.org/), but it's more fun to understand how to do it yourself (and then maybe use someone else's fancy/high quality implementation).\n",
    "\n",
    "First thing to remember is that everything in Python is an object, even functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x\n",
    "\n",
    "isinstance(f, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(isinstance, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(object, object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you create an object, it lives somewhere on your computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(f) # memory address on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1000\n",
    "id(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if two variables are referring to the same address using `is`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x\n",
    "print(\"equality:     {}\".format(z == x))\n",
    "print(\"same address: {}\".format(z is x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 1000\n",
    "print(\"equality:     {}\".format(y == x))\n",
    "print(\"same address: {}\".format(y is x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate functions\n",
    "\n",
    "Let's consider functions that send a real number to a real number\n",
    "$$f:\\mathbb{R} \\to \\mathbb{R}$$\n",
    "Perhaps these functions have some parameters $\\theta$, such as\n",
    "$$f(x; \\theta) = \\theta x$$\n",
    "(a linear function with slope $\\theta$), or\n",
    "$$g(x;\\theta) = \\theta_1 x + \\theta_0$$\n",
    "(linear function with slope $\\theta_1$ and intercept $\\theta_0$), or\n",
    "$$h(x;\\theta) = \\theta_0 \\exp(-\\theta_1 x^2)$$\n",
    "and so on.  The point is that we can parameterize functions that have a similar form, and that there may be different numbers of parameters depending on the function.\n",
    "\n",
    "What might we want to be able to do with a function?\n",
    "1. Evaluate it (`y = f(x)`)\n",
    "2. Print it as a string `f(x) = \"3x + 2\"`\n",
    "3. Calculate a gradient\n",
    "4. add/multiply/exponentiate...\n",
    "\n",
    "We could think of doint the above with methods like `f.evaluate(x)`, and `f.name()`, but we'll use the special methods `__call__` and `__str__` to be able to do things like call `f(x)` and `format(f)` just as we might do so with built-in objects.  You can see the different special methods available to overload [here](https://docs.python.org/3/reference/datamodel.html)\n",
    "\n",
    "We're going to create an abstract function class that all the other classes we create will inherit from.  If you haven't seen object oriented programming before, think of this as a way to promise all our functions will be able to do certain things (or throw an error).  We'll provide default implementations for some methods (these will get filled in later), and have some methods that will need to be implemented differently for each sub-class.\n",
    "\n",
    "For more on classes and inheritance, see [here](https://thepythonguru.com/python-inheritance-and-polymorphism/).  The idea of giving objects methods with the same name is one form of [polymorphism](https://stackoverflow.com/questions/1031273/what-is-polymorphism-what-is-it-for-and-how-is-it-used) - we'll see how this is actually quite useful and allows you to do things that would be difficult without object-oriented programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractUnivariate:\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def fmtstr(self, x=\"x\"):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.fmtstr(\"x\")\n",
    "        \n",
    "    def gradient(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # the rest of these methods will be implemented when we write the appropriate functions\n",
    "    def __add__(self, other):\n",
    "        return SumFunction(self, other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return ProdFunction(self, other)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return ScaleFunction(other, self)\n",
    "    \n",
    "    def __pow__(self, n):\n",
    "        return ComposeFunction(PowerFunction(1, n), self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to create a class that inherits from our abstract class, we just use the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantFunction(AbstractUnivariate): # AbstractUnivariate indicates class to use for inheritance\n",
    "    def __init__(self, c):\n",
    "        self.c = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ConstantFunction(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there's a class hierarchy now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(isinstance(f, ConstantFunction))\n",
    "print(isinstance(f, AbstractUnivariate))\n",
    "print(isinstance(f, object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we haven't implemented the methods we promised we would, we'll get errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead an implement the promised methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantFunction(AbstractUnivariate):\n",
    "    def __init__(self, c):\n",
    "        self.c = c\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.c\n",
    "    \n",
    "    def fmtstr(self, x=\"x\"):\n",
    "        return \"{}\".format(self.c)\n",
    "    \n",
    "    # __str__(self) uses default from abstract class\n",
    "    \n",
    "    def gradient(self):\n",
    "        return ConstantFunction(0)\n",
    "    \n",
    "    # we inherit the other functions from the AbstractUnivariate class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ConstantFunction(3)\n",
    "print(f)\n",
    "print(f(1))\n",
    "print(f(2))\n",
    "print(f.gradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is it this object does?  It represents the constant function\n",
    "$$f: x \\mapsto c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something a little less trivial.  Now we'll implement\n",
    "$$f: x \\mapsto ax + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineFunction(AbstractUnivariate):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.a * x + self.b\n",
    "    \n",
    "    def fmtstr(self, x=\"x\"):\n",
    "        s = \"{}\".format(x)\n",
    "        if self.a != 1:\n",
    "            s = \"{}*\".format(self.a) + s\n",
    "        if self.b != 0:\n",
    "            s = s + \" + {}\".format(self.b)\n",
    "        return s\n",
    "    \n",
    "    def gradient(self):\n",
    "        return ConstantFunction(self.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = AffineFunction(1, 1)\n",
    "print(f)\n",
    "print(f(2))\n",
    "print(f.gradient())\n",
    "print(isinstance(f, AbstractUnivariate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Implement classes for the following univariate function templates:\n",
    "1. `QuadraticFunction` -- $f: x \\mapsto a x^2 + bx + c$\n",
    "2. `ExponentialFunction` -- $f: x \\mapsto a e^{bx}$\n",
    "3. `PowerFunction` -- $f: x \\mapsto ax^n$\n",
    "\n",
    "Make sure to return derivatives that are also `AbstractUnivariate` sub-classes.  Which class can I use to represent $f: x \\mapsto x^{-1}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from math import * # for math.exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More functions\n",
    "\n",
    "We can do more than just encode standard functions - we can scale, add, multiply, and compose functions.\n",
    "\n",
    "Scaling a function:\n",
    "$$ g(x)= a *f(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleFunction(AbstractUnivariate):\n",
    "    def __init__(self, a, f):\n",
    "        self.a = a\n",
    "        if isinstance(f, AbstractUnivariate):\n",
    "            self.f = f\n",
    "        else:\n",
    "            raise AssertionError(\"must input an AbstractUnivariate function\")\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.a * self.f(x)\n",
    "    \n",
    "    def fmtstr(self, x=\"x\"):\n",
    "        if self.a == 1:\n",
    "            return self.f.fmtstr(x)\n",
    "        else:\n",
    "            return \"{}*({})\".format(self.a, self.f.fmtstr(x))\n",
    "    \n",
    "    def gradient(self):\n",
    "        return ScaleFunction(self.a, self.f.gradient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ExponentialFunction(1, 2)\n",
    "print(f)\n",
    "g = ScaleFunction(2, f)\n",
    "print(g)\n",
    "print(g.gradient())\n",
    "print(g(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum and product of two functions\n",
    "$$ h(x) = f(x) + g(x)$$\n",
    "$$ h(x) = f(x) * g(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumFunction(AbstractUnivariate):\n",
    "    def __init__(self, f, g):\n",
    "        if isinstance(f, AbstractUnivariate) and isinstance(g, AbstractUnivariate):\n",
    "            self.f = f\n",
    "            self.g = g\n",
    "        else:\n",
    "            raise AssertionError(\"must input AbstractUnivariate functions\")\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.f(x) + self.g(x)\n",
    "    \n",
    "    def fmtstr(self, x=\"x\"):\n",
    "        return \"{} + {}\".format(self.f.fmtstr(x), self.g.fmtstr(x))\n",
    "    \n",
    "    def gradient(self):\n",
    "        return SumFunction(self.f.gradient(), self.g.gradient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ExponentialFunction(1, 2)\n",
    "g = AffineFunction(2, 1)\n",
    "h = SumFunction(f, g)\n",
    "print(h.fmtstr(x=\"y\"))\n",
    "print(h(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h.gradient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProdFunction(AbstractUnivariate):\n",
    "    def __init__(self, f, g):\n",
    "        if isinstance(f, AbstractUnivariate) and isinstance(g, AbstractUnivariate):\n",
    "            self.f = f\n",
    "            self.g = g\n",
    "        else:\n",
    "            raise AssertionError(\"must input AbstractUnivariate functions\")\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.f(x) * self.g(x)\n",
    "    \n",
    "    def fmtstr(self, x=\"x\"):\n",
    "        return \"({}) * ({})\".format(self.f.fmtstr(x=x), self.g.fmtstr(x=x))\n",
    "    \n",
    "    # product rule (f*g)' = f'*g + f*g'\n",
    "    def gradient(self):\n",
    "        return SumFunction(ProdFunction(self.f.gradient(),self.g), ProdFunction(self.f, self.g.gradient()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ExponentialFunction(1, 2)\n",
    "g = AffineFunction(2, 1)\n",
    "h = ProdFunction(f, g)\n",
    "print(h)\n",
    "print(h(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h.gradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose Functions:\n",
    "$$h(x) = (g \\circ f)(x) = g(f(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComposeFunction(AbstractUnivariate):\n",
    "    def __init__(self, g, f):\n",
    "        if isinstance(f, AbstractUnivariate) and isinstance(g, AbstractUnivariate):\n",
    "            self.f = f\n",
    "            self.g = g\n",
    "        else:\n",
    "            raise AssertionError(\"must input AbstractUnivariate functions\")\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.g(self.f(x))\n",
    "    \n",
    "    def fmtstr(self, x=\"x\"):\n",
    "        return self.g.fmtstr(x=\"({})\".format(self.f.fmtstr(x)))\n",
    "    \n",
    "    # chain rule : g(f(x))' = g'(f(x))*f'(x)\n",
    "    def gradient(self):\n",
    "        return ProdFunction(ComposeFunction(self.g.gradient(), self.f), self.f.gradient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = PowerFunction(1,2)\n",
    "print(f.fmtstr(\"x\"))\n",
    "g = ComposeFunction(f,f)\n",
    "print(g)\n",
    "h = ComposeFunction(g, f)\n",
    "print(h)\n",
    "print(h(2)) # 2^(2*2*2) = 2^8 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = PowerFunction(1,2)\n",
    "g = ExponentialFunction(0.5, -1)\n",
    "h = ComposeFunction(g, f)\n",
    "print(h)\n",
    "print(h.gradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operator overloading makes everything better\n",
    "\n",
    "Recall how when we wrote the AbstractUnivariate class, we included some default methods\n",
    "```python\n",
    "class AbstractUnivariate:\n",
    "    # ...\n",
    "    \n",
    "    # the rest of these methods will be implemented when we write the appropriate functions\n",
    "    def __add__(self, other):\n",
    "        return SumFunction(self, other)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        return ProdFunction(self, other)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return ScaleFunction(other, self)\n",
    "    \n",
    "    def __pow__(self, n):\n",
    "        return ComposeFunction(PowerFunction(1, n), self)\n",
    "```\n",
    "\n",
    "If you think it is clunky to keep writing `SumFunction` or `ProdFunction` everywhere, you're not alone.  Again, you can use the special methods above to [overload operators](https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ExponentialFunction(1, 2)\n",
    "g = AffineFunction(2, 1)\n",
    "print(\"f = {}\".format(f))\n",
    "print(\"g = {}\".format(g))\n",
    "print(\"f + g = {}\".format(f+g))\n",
    "print(\"f * g = {}\".format(f*g))\n",
    "print(\"f^2 = {}\".format(f**2))\n",
    "print(\"2*g = {}\".format(2*g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ExponentialFunction(1, 2)\n",
    "g = AffineFunction(2, 1)\n",
    "h = f*g\n",
    "print(h.gradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's going on?\n",
    "\n",
    "Because we thought ahead to define addition, multiplication, scaling, and powers in our `AbstractUnivariate` class, every sub-class will implement those methods by default **without needing to write any extra code**.\n",
    "\n",
    "If we hadn't done this, we would have had to copy and paste the same thing into every class definition to get the same behavior, **but we don't need to**.  In fact, if we write a new basic univariate function class, e.g. `LogFunction`, we get addition, multiplication, etc., for free!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic Functions\n",
    "\n",
    "Just for fun, let's create an `AbstractUnivariate` sub-class, which just holds a placeholder symbolic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolicFunction(AbstractUnivariate):\n",
    "    def __init__(self, name):\n",
    "        if isinstance(name, str):\n",
    "            self.name=name\n",
    "        else:\n",
    "            raise AssertionError(\"name must be string\")\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return \"{}({})\".format(self.name, x)\n",
    "    \n",
    "    def fmtstr(self, x=\"x\"):\n",
    "        return self.name + \"({})\".format(x)\n",
    "    \n",
    "    # product rule (f*g)' = f'*g + f*g'\n",
    "    def gradient(self):\n",
    "        return SymbolicFunction(self.name + \"'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = SymbolicFunction(\"f\")\n",
    "print(f)\n",
    "print(f.gradient())\n",
    "g = SymbolicFunction(\"g\")\n",
    "print(g  + f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remind ourselves of product rule, and chain rule (which we encoded in `ProductFunction` and `ComposeFunction` classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = SymbolicFunction(\"f\")\n",
    "g = SymbolicFunction(\"g\")\n",
    "print((f*g).gradient())\n",
    "h = ComposeFunction(g, f)\n",
    "print(h.gradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can derive quotient rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = SymbolicFunction(\"f\")\n",
    "g = SymbolicFunction(\"g\")\n",
    "h = f * g**-1\n",
    "print(h)\n",
    "print(h.gradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add symbolic functions to non-symbolic ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = SymbolicFunction(\"f\")\n",
    "g = AffineFunction(1, 2)\n",
    "h = f + g\n",
    "print(h)\n",
    "print(h.gradient())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You're now on your way to having your own automatic differentiation library!  Or your own symbolic computation library!  You can probably see lots of ways to extend and improve what you've seen here:\n",
    "* Support Multivariate Functions\n",
    "* Add more \"basic functions\" such as trig functions, etc.\n",
    "* Reduce expressions when you are able to\n",
    "* ...\n",
    "\n",
    "Yes, there are many libraries that do this very thing.  Keywords are \"autodifferentiation\", \"symbolic math\".  This sort of thing is used extensively in deep learning libraries, as well as optimization libraries.\n",
    "\n",
    "### How was Object Oriented Programming Useful?\n",
    "\n",
    "**Class Inhertiance** allowed you to get functions like addition and multiplication for free once you defined the class everything inherited from\n",
    "\n",
    "**Polymorphism** enabled you to use any combination of `AbstractUnivariate` functions and still evaluate them, calculate derivatives, and format equations. Everyone played by the same rules.\n",
    "\n",
    "**Encapsulation** let you interact with functions without worrying about how they are implemented under the hood.\n",
    "\n",
    "If you think back to HW1, we implicitly used polymorphism in the power method function (e.g., matrix-vector multiply always uses `dot()` no matter which class we're using)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Ignoring our `SymbolicFunction` class, any sub-class of `AbstractUnivariate` is a real function $f:\\mathbb{R} \\to \\mathbb{R}$ that we can evaluate using `f(x)` syntax.  One thing that you may wish to do is find roots of your function: $\\{x \\mid f(x) = 0\\}$.\n",
    "\n",
    "One very classical algorithm for doing this is called [Newton's Method](https://en.wikipedia.org/wiki/Newton%27s_method), and has the basic pseudocode:\n",
    "```\n",
    "initialize x_0\n",
    "while not converged:\n",
    "    x_{k+1} = x_k - f(x_k)/f'(x_k)\n",
    "```\n",
    "\n",
    "Write a function that implements Newton's method on any `AbstractUnivariate` function\n",
    "\n",
    "Hint: use the `gradient()` method to get a function for derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(f, x0=0.0, tol=1e-8):\n",
    "    if isinstance(f, SymbolicFunction):\n",
    "        raise AssertionError(\"can't handle symbolic input\")\n",
    "    elif not isinstance(f, AbstractUnivariate):\n",
    "        raise AssertionError(\"Input must be AbstractUnivariate\")\n",
    "    x = x0\n",
    "    # your code here\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "After the first part of this lecture, you now have a pretty good idea of how to get started implementing a deep learning library.  Recall that above we considered functions of the form\n",
    "$$f(x; \\theta): \\mathbb{R} \\to \\mathbb{R}$$\n",
    "\n",
    "To get to machine learning, you need to handle multivariate input and output\n",
    "$$f(x; \\theta):\\mathbb{R}^p \\to \\mathbb{R}^k$$\n",
    "You also need to be able to take the gradient of $f$ with respect to the parameters $\\theta$ (which we didn't do in our `AbstractUnivariate` class, but is straightforward), and then you can do things like optimize a loss function using your favorite optimization algorithm.\n",
    "\n",
    "In deep learning, we have the exact same setup\n",
    "$$f(x; \\theta):\\mathbb{R}^p \\to \\mathbb{R}^k$$\n",
    "What makes deep learning a \"special case\" of machine learning is that the function $f$ is the composition of several/many functions\n",
    "$$f = f_n \\circ f_{n-1} \\circ \\dots \\circ f_1$$\n",
    "This is what we mean by \"layers\", and you use chain rule to \"backpropagate\" gradients with respect to the parameters.\n",
    "\n",
    "**Disclaimer** If you really want to learn to use a deep learning library, you really should go through several tutorials and learn about the different functions that are used (and *why* they are used).  This is beyond the scope of this course, but there are several courses at Stanford that are devoted to this.\n",
    "\n",
    "## Deep Learning Libraries\n",
    "\n",
    "Some popular libraries for deep learning are [Tensorflow](https://www.tensorflow.org/), [Keras](https://keras.io/), and [PyTorch](https://pytorch.org/).  Each has their strengths and weaknesses.  All of them do essientially the same thing: you define a function through composition using objects that are in many ways similar to what you just implemented.  Then you choose a loss function and start optimizing the parameters in these functions using something like stochastic gradient descent.\n",
    "\n",
    "We'll do an example in PyTorch, since it is higher-level than Tensorflow, and perhaps the most \"Pythonic\" of the libraries.\n",
    "\n",
    "```bash\n",
    "conda install pytorch pillow\n",
    "```\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "What's a tensor?  Conceptually identical to numpy array.\n",
    "\n",
    "We'll consider the following network\n",
    "$$ x \\xrightarrow{w_1} h \\to ReLU(h) \\xrightarrow{w_2} y$$\n",
    "where $x$ is a 500-dimensional vector, $h$ is a 100-dimensional \"hidden layer\", and $y$ is a 10-dimensional vector.  $w_1$ and $w_2$ are linear transformations (matrices), and ReLU refers to the function\n",
    "$$ReLU(x) = \\begin{cases}\n",
    "x & x > 0\\\\\n",
    "0 & x \\le 0\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "# N - batch size\n",
    "# D_in - x dimension\n",
    "# H - h dimension\"\n",
    "# D_out - y dimension\n",
    "N, D_in, H, D_out = 64, 500, 100, 10\n",
    "\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients w.r.t var\n",
    "# during the backward pass.\n",
    "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad = False)\n",
    "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad = False)\n",
    "\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Variables during the backward pass.\n",
    "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "for t in range(10000):\n",
    "  # Forward pass: compute predicted y using operations on Variables;\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2) # clamp=ReLU\n",
    "  \n",
    "  # Compute and print loss using operations on Variables.\n",
    "  # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "\n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Variables with requires_grad=True.\n",
    "    loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent; w1.data and w2.data are Tensors,\n",
    "  # w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are\n",
    "  # Tensors.\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    print(\"Loss is: {}\".format(loss.data.numpy()), end='\\r')\n",
    "\n",
    "print()\n",
    "print(\"Final loss is {}\".format(loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's still fairly cumbersome\n",
    "\n",
    "- When building neural networks, arrange the computation into layers, some of which have learnable parameters which will be optimized during learning.\n",
    "- Use the ``` torch.nn ``` package to define your layers\n",
    "- Create custom networks by subclassing the nn.Module\n",
    "- Really clean code!\n",
    "- Just create a class subclassing the nn.Module\n",
    "    - specify layers in the ```__init__``` \n",
    "    - define a forward pass by ```forward(self,x)``` method\n",
    "    \n",
    "This is analgous to how we created specific sub-classes of `AbstractUnivariate`, and got a lot for free through class inheritance, polymorphism, abstraction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, H, D_out): # this defines the parameters, and stores them\n",
    "        super(TwoLayerNet, self).__init__() # overrides class inheritance\n",
    "        self.layer1 = nn.Linear(D_in, H) # initializes weights\n",
    "        self.layer2 = nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x): # this defines the composition of functions\n",
    "        out = F.relu(self.layer1(x)) \n",
    "        out = self.layer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension; H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out) # we create our function f:x \\to y\n",
    "\n",
    "# Construct our loss function and an Optimizer. \n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(1000):\n",
    "  # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x) # evaluate the f(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y) # evaluate the loss\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(\"Final Loss is {}\".format(loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For more examples... \n",
    "- check out [Pytorch Docs](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "\n",
    "## Object Oriented Programming\n",
    "\n",
    "* Beginner's guide to Object Oriented Programming in Python [here](https://stackabuse.com/object-oriented-programming-in-python/)\n",
    "\n",
    "## Image Processing\n",
    "\n",
    "In this class, we've worked a lot with tabular data.  Another important type of data to be able to work with is image data.\n",
    "\n",
    "Some option are\n",
    "* [scikit-image](https://scikit-image.org/)\n",
    "* [scipy](http://www.scipy-lectures.org/advanced/image_processing/index.html)\n",
    "* [Pillow](https://pillow.readthedocs.io)\n",
    "* [OpenCV](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)\n",
    "\n",
    "For many examples, see the [Scikit-image gallery](http://scikit-image.org/docs/stable/auto_examples/).  Other libraries also have examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cme193-3.6]",
   "language": "python",
   "name": "conda-env-cme193-3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
